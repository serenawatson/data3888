{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79ba5a03",
   "metadata": {},
   "source": [
    "# DATA3888 Project Report: Holiday Planner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6deb6cd7",
   "metadata": {},
   "source": [
    "COVID C4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d046a544",
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'requests_html'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-926bc4af5e76>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mticker\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMaxNLocator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0manalytics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manalytics_clustering\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0manalytics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manalytics_helper_clustering\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0manalytics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manalytics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\data3888-covid-c4\\analytics.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0manalytics_helper\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcommon\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\data3888-covid-c4\\analytics_helper.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimedelta\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mrequests_html\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mHTMLSession\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mrequests_html\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mHTML\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecomposition\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPCA\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'requests_html'"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import more_itertools\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from statistics import mean\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "from analytics.analytics_clustering import *\n",
    "from analytics.analytics_helper_clustering import *\n",
    "from analytics.analytics import *\n",
    "from analytics.analytics_helper import *\n",
    "from analytics.common import *\n",
    "\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2db88c",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# hides warnings - these warnings do not affect code functionality\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3c2828",
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "random.seed(3888)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6554b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading in all data\n",
    "covid = read_live_covid_data()\n",
    "countries_data = integrate_all_data(covid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df597be",
   "metadata": {},
   "source": [
    "## 1. Executive summary\n",
    "\n",
    "Due to the travel industry being heavily affected by COVID-19, our ‘Holiday Planner’ application was proposed to incorporate the pandemic with the tourism business to show Australians that traveling can still be the rewarding experience that it once used to be. This report encapsulates the major problems, our main findings/solutions, and the practical relevance of our analysis.\n",
    "\n",
    "### 1.1. Problems\n",
    "\n",
    "With quarantine restrictions lifted and even with international borders opening, Australians are still too scared to travel because they are not aware of their options (Crismale, 2022). The post-pandemic world brings new challenges to international travel with more costs and the necessity of meeting all pre-departure requirements. The Australian government has produced their own app - Smartraveller - to battle this issue and help Australians get all the information they need before they travel, however their app does have its shortcomings. For example, if they want to travel somewhere which is not currently suitable for travel, how would they know what other possible locations to visit to get a similar experience? So, currently the main challenge is recommending Australian people with the best overseas travel destinations, regardless of if they have any initial interests or not.\n",
    "\n",
    "### 1.2. Solution/Findings\n",
    "\n",
    "As an answer to the problem outlined above, we decided on a recommendation software based on machine learning algorithms using OWID COVID data and a diverse range of travel data gathered from Smarttraveller RSS feed, Triposo API, etc. \n",
    "\n",
    "Our Holiday Planner app incorporated two modes - an ensemble 10-NN model, and a size-constrained k-means clustering model. If the user chose to specify a country of interest, we used the 10-NN model and returned the top 10 similar countries. If the user didn’t specify the country, we performed size-constrained k-means clustering to determine the 10-12 countries most suited to their interests. \n",
    "\n",
    "Due to the subjective nature of our recommender, it was difficult for us to evaluate the ‘accuracy’ of our results, hence we decided to focus on evaluating our methods. To evaluate our KNN method, we calculated the similarity score to explore the stability of our model. It was found that we had an average similarity score of 0.97 between all models, proving that our model was quite stable. To evaluate our clustering, we decided to utilise a silhouette score to determine the model’s goodness of fit which was found to be around 0.36. Although this showed that our clusters were different from one another, they were not clearly distinguished from one another.\n",
    "\n",
    "### 1.3. Practical Relevance\n",
    "\n",
    "The practical relevance of our analysis was that we used our results to power our ‘Holiday Planner’ recommendation dashboard. It is catered towards Australians who are interested in post-covid international travel and has the ability to personalise recommendations based on specific countries or their personal factors of interest. For future works, it can be further tweaked to provide recommendations based on cities rather than countries if there is access to quality and relevant data down to city level."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dca9575",
   "metadata": {},
   "source": [
    "## 2. Aim and Background\n",
    "\n",
    "With COVID-19 lockdown restrictions ending and international borders opening, Australians can once again enjoy the luxury of travelling overseas. The aim of this project is to help fellow Australians choose a holiday destination based on factors that comprise of both safety and enjoyment.\n",
    "\n",
    "Although the pandemic seems to be easing out here in Australia, it cannot be said that it is the same everywhere. Many countries are still suffering under the hands of this epidemic and Australians now being wary of travel due of safety issues, therefore we wanted to ensure that our users were aware of the current conditions of the location they wanted to visit. Moreover, we understand that due to COVID-19 cases and travel restrictions that are still yet to be lifted in many countries, they might not be able to travel to their destination of choice. Hence, we also wanted to provide recommendations based on their dream destination and/or factors (even outside of COVID-19) that travellers in general are interested in - including but not limited to nature, food, infrastructure, arts, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e851480",
   "metadata": {},
   "source": [
    "## 3. Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701aa095",
   "metadata": {},
   "source": [
    "### 3.1. Question formulation\n",
    "Initially we were inspired by the OWID data to plan to analyse things like the effect of variant dominance on vaccination effectiveness, however, later we realized that facts about COVID-19 or vaccinations may no longer be the centre topic of normal people, as COVID-19 has been existing for more than 2 years, there are already different types of dashboards around the world for people to obtain the latest information about COVID, and for the policy makers, they need more sophisticated research on COVID and vaccinations which are more suitable for epidemiologists and pharmacists to conduct, therefore, we as a interdisciplinary group which mainly consists of people from data science filed, decided to look at systems that have been affected by COVID-19 and explore what we can do for the post-COVID world. \n",
    "\n",
    "We firstly listed several affected systems including Food(restaurants/take-outs), Entertainment(music festivals/concerts/plays), Travel, Employment and Transportation. After discussions and brief investigations, we have chosen to focus on the travel system among those affected systems. The key factor in making that choice is, for other affected systems, we got difficulty in collecting diverse and useful data, besides, it is noticeable that there has been severe declines in global mobility. In past 2 years, many countries temporarily locked their borders while also carrying out different levels of lockdowns within borders to prevent epidemic outbreaks, hence, under the situation that finally the world is reopening, people are eager to plan their tours. So we started to aim at making an application for people nowadays to choose the most satisfying travelling destinations. As the requirements for Australian citizens and permanent residents to get the individual travel exemption have been relaxed since April (Australian Government, 2022), Australians are free to go for an outside-border travel in most cases, therefore, Australian residents are supposed to be the most suitable users for this application. \n",
    "\n",
    "Once our main aim and target audience have been decided, the main question of our project is also formulated - how to recommend Australian people with the best overseas travel destinations no matter they have initial interests or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7374ea34",
   "metadata": {},
   "source": [
    "### 3.2. Preparing the data for modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66c97ac",
   "metadata": {},
   "source": [
    "In addition to the live COVID dataset from OWID, we collected a diverse range of external datasets, listed below:\n",
    "\n",
    "- Travel advice data from Smartraveller RSS feed\n",
    "- Point of interest ratings data from Triposo API\n",
    "- Country descriptions from Triposo API\n",
    "- Tourism indexes dataset from Travel & Tourism Competitiveness Report\n",
    "- Country photos from Google Places API\n",
    "\n",
    "In terms of the live COVID data, we only used data from the last 30 days, to ensure our recommendations were up-to-date. For each country, we computed the median of each COVID variable over the last 30 days, as median is a robust measure of centre.\n",
    "\n",
    "Once all datasets had been integrated, we weighted by a factor of 1000 the features which the user was interested in. Importantly, in our UI, the user did not directly select individual features in our dataset; rather, they selected \"variable groups\", where each variable group corresponded to several features in our dataset. For example, the COVID variable group included the variables `new_cases_smoothed_per_million` and `new_deaths_smoothed_per_million` from our integrated dataset. Increasing the weighting of features that the user was interested in ensured our recommendations placed more emphasis on these features, and thus were tailored to user preferences.\n",
    "\n",
    "We then performed PCA and extracted the first and second principal components, to avoid the curse of dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff910075",
   "metadata": {},
   "source": [
    "### 3.3. Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7539f180",
   "metadata": {},
   "source": [
    "Our Holiday Planner app incorporated two models - an ensemble 10-NN model, and a size-constrained k-means clustering model.\n",
    "\n",
    "#### 3.3.1. Ensemble 10-NN\n",
    "\n",
    "In our UI, if the user did select a country of interest, we used ensemble 10-NN to determine the 10 most similar countries.\n",
    "\n",
    "Our ensemble 10-NN model consisted of 9 10-NN models, each of which used a different distance metric. The 9 distance metrics we used are listed below:\n",
    "\n",
    "1. Euclidean\n",
    "2. Manhattan\n",
    "3. Chebyshev\n",
    "4. Cosine\n",
    "5. Cityblock\n",
    "6. Braycurtis\n",
    "7. Canberra\n",
    "8. Correlation\n",
    "9. Minkowski\n",
    "\n",
    "Each 10-NN model was given the country selected by the user, and returned this country's 10 nearest neighbours (10 most similar countries).\n",
    "\n",
    "After each 10-NN model had been run for the user-specified country, we considered the corresponding 9 sets of neighbours. We then returned the 10 neighbours which were the most common across all 9 neighbour sets, as the final recommendations to the user.\n",
    "\n",
    "#### 3.3.2. Size-constrained k-means clustering\n",
    "\n",
    "In our UI, if the user did not select a country of interest, we used size-constrained k-means clustering across all countries in our dataset to determine the 10-12 countries most suited to their interests.\n",
    "\n",
    "For each cluster produced by size-constrained k-means clustering, we determined its average rating across all interests specified by the user (if the user did not specify any interests, we took the average rating across _all_ features in our dataset). For all possible interests except COVID, a higher rating meant the country had a higher standard for this interest. However, for COVID, clearly _lower_ case/death numbers are preferable. Thus, for each COVID variable (cases, deaths), the value for each country was subtracted from the max value for this variable. This transformation ensured that higher values (differences from max) were preferable - consistent with all other variables. We then returned the cluster with the highest average rating as countries recommended to the user.\n",
    "\n",
    "In terms of what size-constrained k-means clustering is, it is a modification of k-means clustering which would allow a minimum cluster size to be specified (Bradley et al., 2000). The package we used to perform size-constrained k-means clustering, k-means-constrained (Levy-Kramer, 2022), extends upon the work of Bradley et al. (2000) by also allowing a _maximum_ cluster size to be specified. As explained above, our recommendations to the user are the single \"best\" cluster, and so we specify a minimum cluster size of 10 and a maximum of 12, since this would give the user a certain degree of variety without overwhelming them with too many options."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237fecdb",
   "metadata": {},
   "source": [
    "The below figure summarises our modelling approach:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e48ee81",
   "metadata": {},
   "source": [
    "<img src=\"fig/MethodFigure.png\" alt=\"Modelling approach\">\n",
    "\n",
    "_Figure 1: Our modelling approach_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81acd3f2",
   "metadata": {},
   "source": [
    "### 3.4. Evaluation strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71d0d22",
   "metadata": {},
   "source": [
    "When testing the validity of our models, we decided on three ways to evaluate our process. \n",
    "For 10-NN, we calculated the average proportion of similarities between results, when any one distance metric was removed from the ensemble. While for clustering, we calculated the average silhouette score. For both these strategies we decided to take a random sample of user input combinations as the total combinations for our user inputs is too large. As a qualitative measure, we also compared our result to online travel recommendations webistes and blogs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c330cd5f",
   "metadata": {},
   "source": [
    "## 4. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bb6128",
   "metadata": {},
   "source": [
    "### 4.1. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9589bcc8",
   "metadata": {},
   "source": [
    "It was difficult to evaluate the accuracy of our results due to the subjective nature of the recommender. Therefore we decided to focus on evaluating our methods. \n",
    "\n",
    "#### 4.1.1. Evaluating KNN\n",
    "When evaluating our 10 nearest-neighbours ensemble, we decided to calculate the similarity score and explore how stable our model was. Due to the numerous combinations of user inputs we may get, we had to take a random sample of user inputs. Therefore we calcuated the country recommendations for a randomly selected combination of 5 countries and 3 interest inputs, 15 samples all together. For each combination of inputs, we then calculated the recommendation with both the full model and when one distance metric was removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b424928",
   "metadata": {
    "code_folding": [],
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "# example of one case of similarity testing for the ensemble\n",
    "\n",
    "num_neighbours = 10\n",
    "iso_location = read_iso_loc_data()\n",
    "\n",
    "all_metrics = ['braycurtis', 'canberra', 'chebyshev', 'cityblock', 'correlation',\n",
    "               'cosine', 'euclidean', 'manhattan', 'minkowski']\n",
    "\n",
    "interests = [\"covid\", \n",
    "             \"infrastructure_quality_and_availability\",\n",
    "             \"health_and_safety\",\n",
    "             \"cost\",\n",
    "             \"fun\",\n",
    "             \"nature\",\n",
    "             \"food\",\n",
    "             \"museums\",\n",
    "             \"showstheatresandmusic\",\n",
    "             \"wellness\",\n",
    "             \"wildlife\"]\n",
    "\n",
    "regions = ['Asia-Pacific', 'Europe and Africa', 'Americas']\n",
    "\n",
    "# randomly selecting 3 interest combinations (only 3 to save time - more takes too long to run)\n",
    "\n",
    "interest_combs_10NN = list(more_itertools.powerset(interests))\n",
    "\n",
    "interest_combs_10NN_chosen = random.sample(interest_combs_10NN, 3)\n",
    "\n",
    "while [False] * 15 in interest_combs_10NN_chosen:\n",
    "    interest_combs_10NN_chosen = random.sample(interest_combs, 3)\n",
    "    \n",
    "# randomly choosing 5 countries\n",
    "\n",
    "countries_10NN_chosen = random.sample(list(iso_location['location'].drop_duplicates()), 5)\n",
    "\n",
    "# computing proportion of similarity for each metric, across different country and interest combinations\n",
    "\n",
    "prop_similarity = {}\n",
    "\n",
    "for metric in all_metrics:\n",
    "    dist_metrics = list(set(all_metrics) - set([metric]))\n",
    "    regions = ['Asia-Pacific', 'Europe and Africa', 'Americas']\n",
    "    \n",
    "    for interests in interest_combs_10NN_chosen:  \n",
    "    \n",
    "        if len(interests) > 0 and len(regions) > 0:\n",
    "            continents = convert_regions_to_continents(regions)\n",
    "\n",
    "            cols_of_interest = convert_interests_to_cols(interests)\n",
    "            weightings = generate_feature_weightings_dict(cols_of_interest)\n",
    "\n",
    "            for country in countries_10NN_chosen:  \n",
    "                medians_scaled, medians, data_no_quant = prepare_data_for_nn(countries_data, country, continents, weightings)\n",
    "\n",
    "                if not loc_to_iso_code(country, iso_location) in medians.index:\n",
    "                    continue\n",
    "\n",
    "                # computing neighbour set using all metrics\n",
    "                neighbours_all_metrics = list(generate_final_df_w_nn(country,\n",
    "                                                                 medians_scaled,\n",
    "                                                                 medians,\n",
    "                                                                 data_no_quant,\n",
    "                                                                 num_neighbours = num_neighbours,\n",
    "                                                                 dist_metrics=all_metrics)['10NN'])[0]\n",
    "\n",
    "                # computing neighbour set using all metrics except \"metric\"\n",
    "                neighbours_metric_missing = list(generate_final_df_w_nn(country,\n",
    "                                                                 medians_scaled,\n",
    "                                                                 medians,\n",
    "                                                                 data_no_quant,\n",
    "                                                                 num_neighbours = num_neighbours,\n",
    "                                                                 dist_metrics=dist_metrics)['10NN'])[0]\n",
    "\n",
    "                # computing intersection of the two neighbour sets\n",
    "                neighbours_intersect = set(neighbours_all_metrics).intersection(neighbours_metric_missing)    \n",
    "\n",
    "                \n",
    "                # computing proportion of neighbours shared between both sets\n",
    "                sim = len(neighbours_intersect)/num_neighbours\n",
    "\n",
    "                if not metric in prop_similarity:\n",
    "                    prop_similarity[metric] = [sim]\n",
    "                else:\n",
    "                    prop_similarity[metric].append(sim)\n",
    "                    \n",
    "# computing average proportion of similarity, per metric\n",
    "\n",
    "avg_prop_similarity = {}\n",
    "\n",
    "for metric in prop_similarity.keys():\n",
    "    avg_prop_similarity[metric] = mean(prop_similarity[metric])\n",
    "    \n",
    "sim_vals = list(avg_prop_similarity.values())\n",
    "\n",
    "# plotting similarity scores across interest and country combinations\n",
    "\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.set_title('Similarity Score')\n",
    "ax1.hist(sim_vals)\n",
    "ax1.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8718e9ca",
   "metadata": {},
   "source": [
    "_Figure 2: A distribution of all collected similarity scores._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc57fb3",
   "metadata": {},
   "source": [
    "Tested across all randomly selected inputs, we found that all scores stayed within a range of 0.95 and 1. Therefore we found an average of 97% similarity between all models after the removal of each distant metric, allowing us to conclude that the model is quite stable and doesn't depend on any one metric.\n",
    "\n",
    "#### 4.1.2. Evaluating Clustering\n",
    "\n",
    "For the size-constrained clustering, we decided to utilise a silhouette score to determine the model’s goodness of fit. Similarly to KNN, we had to randomly select a sample of 100 combination of user interests, combined with all possible combination of region inputs. We then clustered on each of the resulting combinations and collected all silhouette scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d3995d",
   "metadata": {
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "scores = []\n",
    "\n",
    "# randomly choosing 100 interest combinations\n",
    "# each interest combination is essentially a list of 15 Booleans, 1 per interest\n",
    "# each list of Booleans is used to create an interested dict two code blocks down\n",
    "\n",
    "interest_combs = list(itertools.product([False, True], repeat=15))\n",
    "\n",
    "interest_combs_chosen = random.sample(interest_combs, 100)\n",
    "\n",
    "while [False] * 15 in interest_combs_chosen:\n",
    "    interest_combs_chosen = random.sample(interest_combs, 100)\n",
    "    \n",
    "# generating all region combinations\n",
    "\n",
    "region_combs = list(more_itertools.powerset(regions))\n",
    "\n",
    "interests = [\"covid\", \n",
    "             \"infrastructure_quality_and_availability\",\n",
    "             \"health_and_safety\",\n",
    "             \"cost\",\n",
    "             \"fun\",\n",
    "             \"nature\",\n",
    "             \"food\",\n",
    "             \"museums\",\n",
    "             \"showstheatresandmusic\",\n",
    "             \"wellness\",\n",
    "             \"wildlife\"]\n",
    "\n",
    "for interest_comb in interest_combs_chosen:  \n",
    "    for regions in region_combs:\n",
    "        # generating the interested dict corresponding to the given interest combination, interest_comb\n",
    "        interested = {}\n",
    "        for i, interest in enumerate(interests):\n",
    "            interested[interest] = interest_comb[i]\n",
    "            \n",
    "        # computing silhouette score, for given sets of interests and regions\n",
    "        if list(interested.values()).count(True) != 0 and len(regions) != 0:\n",
    "            weightings = convert_interests_to_col_weightings(interested)\n",
    "            medians_scaled_pca, medians_scaled = prepare_data_for_clustering(countries_data, regions, weightings)\n",
    "\n",
    "            scores.append(compute_silhouette_score(medians_scaled_pca))\n",
    "            \n",
    "# plotting silhouette scores across interest and region combinations\n",
    "\n",
    "fig2, ax2 = plt.subplots()\n",
    "ax2.set_title('Silhouette Score')\n",
    "ax2.boxplot(scores, vert=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf9a02a",
   "metadata": {},
   "source": [
    "_Figure 3: A distribution of all collected silhouette scores for the sample._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d3bc56",
   "metadata": {},
   "source": [
    "From the boxplot above we can see that the average silhouette score for the clusters were around 0.36, with the range from 0.25 to 0.6. As no results fell close to or below zero, this would suggests that the clusters are different from one another. However as it is very much below 1, our clusters are still not as clearly distinguished or significantly different from one another. \n",
    "\n",
    "#### 4.1.3. Qualitative Evaluation\n",
    "\n",
    "As a qualitative measure, we also compared our result to travel recommendations made by travel writers and websites. This was to compare our subjective recommendation with popular opinion and suggestions from travel professionals. \n",
    "\n",
    "We compared the countries listed on our recommender with other countries listed on blogs or recommendation websites with similar interests (e.g. travelling in Europe or best countries for museums). Through this we found that our recommendation were often similar to what others were suggesting. While there wasn't 100% similarity, there generally weren't any abnormal suggestions from our algorithm. \n",
    "\n",
    "Although it is worth noting that this evaluation method was purely a sense check. There were many limitations to this as it was difficult to find online recommendations which matched our specific user interests. Therefore it was simply used to double check if we had any outliers in certain suggestions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803df7e5",
   "metadata": {},
   "source": [
    "### 4.2. Deployment Process ####\n",
    "Our deployment process consisted of several steps: planning, development, testing, and deployment of the final product. These steps are closely based on the PMKBOK Process Groups, widely used in project management domains.\n",
    "\n",
    "#### 4.2.1. Planning ####\n",
    "Our planning phase was multifaceted. \n",
    "\n",
    "First, we had to identify and acquire data from multiple sources. This included static sources such as reports, and dynamic constantly updating sources such as APIs and RSS feeds.\n",
    "\n",
    "Then, we formulated the ensemble KNN and k-means size-constrained clustering algorithms. This was done through a combination of our own research and in consultation with the DATA3888 tutoring team. \n",
    "\n",
    "We then prototyped the dashboard design using wireframes and wireflows on paper and subsequently on the Figma web application. Prototyping in this way is a key process in the User Interface and User Experience disciplines. \n",
    "\n",
    "#### 4.2.2. Development #### \n",
    "Our team then split into two discrete teams: an analytics team, and a UI group. We made use of GitHub in order to facillitate code sharing and collaboration. Both teams used a singular repository with multiple branches. \n",
    "\n",
    "The analytics code utilised the scikit-learn, numpy, pandas and k-means-constrained libraries, while the UI component was implemented using the Plotly Dash Python libraries and the Dash Extensions Python library. The UI elements were implemented using the Bulma CSS framework. The dashboard and code are served by a Flask server.\n",
    "\n",
    "The analytics code and basic UI implementation were completed in parallel, with the teams working together in the last stage to implement application callbacks in order to utilise the analytics functionality within the dashboard.\n",
    "\n",
    "\n",
    "#### 4.2.3. Testing ####\n",
    "Testing was performed during all stages of development in an ad-hoc manner. Using a range of inputs into the dashboard, including normal input and edges cases, (nothing selected, everything selected, etc.) subsequent outputs were checked for consistent results and callback errors (provided via the Plotly Dash debug mode).\n",
    "\n",
    "#### 4.2.4. Deployment and Showcase of the Final Product #### \n",
    "After integration and final testing, the server was put into production mode. A feature of the application is that on initial startup, it fetches the most up-to-date information from our data sources. The application is run using `main.py`.\n",
    "\n",
    "A user will be first be greeted with a variety of options allowing them to select a destnation on interest, possible concerns, and travel interests. The subsequent results are based on the interests and concerns that the user selects. The travel recommendation system then provides ten to twelve destination recommendations, with their geographical locations highlighted on a map of the world.\n",
    "\n",
    "<img src=\"fig/Figure 4.png\" alt=\"User selected destinations and factors\">\n",
    "\n",
    "*Figure 4: User selected destination, concerns, and interests and resulting destination suggestions and portraits, with destinations highlighted on the world map.*\n",
    "\n",
    "After this point, a user can click on a destination's portrait to view extra information about a destination, including travel warnings, a short description, covid statistics, and scores relating to the user's selected concerns and interests.\n",
    "\n",
    "<img src=\"fig/Figure 5.png\" alt=\"Extra travel information for Turkey\">\n",
    "\n",
    "*Figure 5: Destination information for Turkey, with travel warnings, a short description, and factor scores.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb7c127",
   "metadata": {},
   "source": [
    "## 5. Discussion/Conclusion\n",
    "\n",
    "One shortcoming of the project's current implementation is the lack of any forecasted covid data. As the dashboard is intended in the use of planning travel, use of current covid statistics may not be reflective of the situation at the intended time of travel. Implementation of forecasting, alongside a visual aid reflecting the confidence of the forecasts into the future would be a useful addition to the project.\n",
    "\n",
    "Issues with data quality, particularly in the OWID covid dataset, resulted in a significant portion of countries being excluded from the dashboard's potential recommended countries. However, since issues with missing values stem from the country in question's reporting, there is little which is possible to remedy this issue. \n",
    "\n",
    "The dashboard currently outputs recommendations on a country level, which matches the granularity of most of the datasets we received. However, some datasets (i.e. Triposo) provide city level data, which may be more useful to our intended audience when planning trips, especially when recommended countries are large or have many cities which vary in character. Implementation of city level recommendations would be a welcome addition in future work, both within a country and across multiple countries.\n",
    "\n",
    "Given that the dashboard currently provides 10 recommended countries in its output, use of the tool in planning holidays spanning multiple destinations is expected. However, additional work could be added in aiding the planning of multi-stop trips. Implementing flight price, geographical and accomodation data for calculation and recommendation of potential multi-stop trips, as well as adding expected price ranges for single and multi-stop trips would be a useful addition.\n",
    "\n",
    "On the clustering side of the analytics, where users do not specify a starting country in which they are interested, more advanced analytical algorithms could have been implemented, i.e. DBSCAN. The implementations of such algorithms may have aided performance, but it is noted that with a silhouette score of 0.3 on our size constrained clustering, our clusters still had good separation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb9344d",
   "metadata": {},
   "source": [
    "## 6. Student contributions\n",
    "\n",
    "### 6.1. Eve Fernando\n",
    "\n",
    "I found and integrated various data sources, including all data obtained from the Triposo API and Smartraveller RSS feed. I also integrated live COVID data from the OWID GitHub repository. I performed data pre-processing, including scaling and PCA. I also implemented feature weighting for the features the user was interested in. In terms of our modelling approach, I found the nearest neighbours and _size-constrained_ k-means clustering algorithms through online research, and subsequently implemented them. I also implemented the logic used to find the \"best\" cluster produced by size-constrained k-means clustering. In terms of evaluation, though Serena Watson completed most of the work in this regard, I also contributed some code to this. In particular, I implemented random selection of interest/country combinations for evaluation of 10-NN/clustering. Additionally, I successfully fixed a UI bug, which involved recommended countries on the right side of the UI changing when an individual country was clicked (desired behaviour was for _no change_ to occur). Furthermore, I worked with Serena Watson to write the presentation script and create the slides. I wrote up sections 3.2 and 3.3 in the report, and generated Figures 2 and 3.\n",
    "\n",
    "### 6.2. Marie Montgomery\n",
    "\n",
    "I contributed to the idea formulation including data collection. I implemented the initial Googles Places API usage, before refinement by Yan. I worked mostly on the UI section of the project, implementing a significant part of the UI including base structure and layout, use of CSS, dropdown menus, a variety of callback functions to interact with the ML/analytics code, session storage, as well as implementation of the functionality of the left info panel for a selected destination. For the report, I contributed section 4.2.\n",
    "\n",
    "### 6.3. Rayani Saha\n",
    "\n",
    "For the analysis part, I helped out in the early stages of our idea formulation - this included creating figures/graphs to show how the variables in our chosen dataset interacted as well writing experimental functions on k-means clustering. Other than that, I worked on data integration for the UI - mainly the home page which included getting the countries to show up, helping Stuart Toft integrate the map function into the callback functions, and bug fixing any errors that occurred. For the presentation, I did the live demo along with Yan Liu. For the report, I wrote the executive summary and the aim/background, as well as editing the report. \n",
    "\n",
    "### 6.4. Serena Watson\n",
    "\n",
    "In terms of contribution, I was involved in the initial idea formulation and data collection phases before branching out to exploring initial analytics options and refine later analytics models used for our recommendation. I was also heavily involved in researching, identifying, and applying strategies to evaluate our product as well as the presentation production and delivery. For the report, I contributed to all evaluation sections, with a focus on 4.1.\n",
    "\n",
    "\n",
    "### 6.5. Stuart Toft\n",
    "\n",
    "### 6.6. Yan Liu\n",
    "\n",
    "I contributed to the idea formulation and I was invovled in the data/information collection. I joined in the earlier stage of analytics work, which is experimenting on different clustering methods, tried the normal k-means cluster with a brief evaluation as a back up. For the dashboard, I firstly made the storyboard for the product, then implemented showing the correspoding photos when the recommendations are returned. I presented the product demonstration with Ray. For the report, I worte up the question formulation which is section 3.1. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599e3861",
   "metadata": {},
   "source": [
    "## 7. References\n",
    "\n",
    "Australian Government. (2022, April 18). International Travel. Official Australian Government Information. https://www.australia.gov.au/international-travel\n",
    "\n",
    "Bradley, P. S., Bennett, K. P., & Demiriz, A. (2000). Constrained k-means clustering. Microsoft Research, Redmond, 20(0), 0.\n",
    "\n",
    "Destinations. Smartraveller. (2022). Retrieved 30 May 2022, from https://www.smartraveller.gov.au/destinations.\n",
    "\n",
    "Europe's Best Destinations. (2022). Best Countries to Visit in Europe in 2022. Retrieved 31 May 2022, from https://www.europeanbestdestinations.com/best-of-europe/best-countries-to-visit-in-europe/.\n",
    "\n",
    "Google Places API. Google Developers. (2022). Retrieved 30 May 2022, from https://developers.google.com/maps/documentation/places/web-service.\n",
    "\n",
    "Laubheimer, P. (2016, December 4). Wireflows: A UX Deliverable for Workflows and Apps. Nielsen Norman Group. https://www.nngroup.com/articles/wireflows/\n",
    "\n",
    "Levy-Kramer, J. (2022). k-means-constrained.\n",
    "\n",
    "Lonely Planet. (2022). Travel Guides & Travel Information  Retrieved 31 May 2022, from https://www.lonelyplanet.com/.\n",
    "\n",
    "Our World in Data, Coronavirus Disease (COVID-19) – the data [Dataset]. https://ourworldindata.org/coronavirus-data\n",
    "\n",
    "Planetware. (2022). 19 Best Countries to Visit in Europe. Retrieved 31 May 2022, from https://www.planetware.com/europe/best-countries-to-visit-in-europe-d-1-19.htm.\n",
    "\n",
    "Reuters. (2011). Travel Picks: Top 10 museum destinations. Retrieved 30 May 2022, from https://www.reuters.com/article/uk-travel-picks-museums-idUSLNE76S01020110729\n",
    "\n",
    "Touropia. (2022). 15 Best Countries to Visit in Asia. Retrieved 31 May 2022, from https://www.touropia.com/best-countries-to-visit-in-asia/.\n",
    "\n",
    "Triposo API. triposo. (2022). Retrieved 30 May 2022, from https://www.triposo.com/api/.\n",
    "\n",
    "Vargas, R. V. (2001). A new approach to PMBOK guide 2000. Paper presented at Project Management Institute Annual Seminars & Symposium, Nashville, TN. Newtown Square, PA: Project Management Institute.\n",
    "\n",
    "World Bank (2019). WEF Travel & Tourism Competitiveness [Dataset]. https://tcdata360-backend.worldbank.org/api/v1/datasets/78/dump.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3a3fe5",
   "metadata": {},
   "source": [
    "## 8. Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6b040d",
   "metadata": {},
   "source": [
    "### 8.1. Code used to generate Figure 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e5f8f5",
   "metadata": {},
   "source": [
    "\n",
    "```python\n",
    "# example of one case of similarity testing for the ensemble\n",
    "\n",
    "num_neighbours = 10\n",
    "iso_location = read_iso_loc_data()\n",
    "\n",
    "all_metrics = ['braycurtis', 'canberra', 'chebyshev', 'cityblock', 'correlation',\n",
    "               'cosine', 'euclidean', 'manhattan', 'minkowski']\n",
    "\n",
    "interests = [\"covid\", \n",
    "             \"infrastructure_quality_and_availability\",\n",
    "             \"health_and_safety\",\n",
    "             \"cost\",\n",
    "             \"fun\",\n",
    "             \"nature\",\n",
    "             \"food\",\n",
    "             \"museums\",\n",
    "             \"showstheatresandmusic\",\n",
    "             \"wellness\",\n",
    "             \"wildlife\"]\n",
    "\n",
    "regions = ['Asia-Pacific', 'Europe and Africa', 'Americas']\n",
    "\n",
    "# randomly selecting 3 interest combinations (only 3 to save time - more takes too long to run)\n",
    "\n",
    "interest_combs_10NN = list(more_itertools.powerset(interests))\n",
    "\n",
    "interest_combs_10NN_chosen = random.sample(interest_combs_10NN, 3)\n",
    "\n",
    "while [False] * 15 in interest_combs_10NN_chosen:\n",
    "    interest_combs_10NN_chosen = random.sample(interest_combs, 3)\n",
    "    \n",
    "# randomly choosing 5 countries\n",
    "\n",
    "countries_10NN_chosen = random.sample(list(iso_location['location'].drop_duplicates()), 5)\n",
    "\n",
    "# computing proportion of similarity for each metric, across different country and interest combinations\n",
    "\n",
    "prop_similarity = {}\n",
    "\n",
    "for metric in all_metrics:\n",
    "    dist_metrics = list(set(all_metrics) - set([metric]))\n",
    "    regions = ['Asia-Pacific', 'Europe and Africa', 'Americas']\n",
    "    \n",
    "    for interests in interest_combs_10NN_chosen:  \n",
    "    \n",
    "        if len(interests) > 0 and len(regions) > 0:\n",
    "            continents = convert_regions_to_continents(regions)\n",
    "\n",
    "            cols_of_interest = convert_interests_to_cols(interests)\n",
    "            weightings = generate_feature_weightings_dict(cols_of_interest)\n",
    "\n",
    "            for country in countries_10NN_chosen:  \n",
    "                medians_scaled, medians, data_no_quant = prepare_data_for_nn(countries_data, country, continents, weightings)\n",
    "\n",
    "                if not loc_to_iso_code(country, iso_location) in medians.index:\n",
    "                    continue\n",
    "\n",
    "                # computing neighbour set using all metrics\n",
    "                neighbours_all_metrics = list(generate_final_df_w_nn(country,\n",
    "                                                                 medians_scaled,\n",
    "                                                                 medians,\n",
    "                                                                 data_no_quant,\n",
    "                                                                 num_neighbours = num_neighbours,\n",
    "                                                                 dist_metrics=all_metrics)['10NN'])[0]\n",
    "\n",
    "                # computing neighbour set using all metrics except \"metric\"\n",
    "                neighbours_metric_missing = list(generate_final_df_w_nn(country,\n",
    "                                                                 medians_scaled,\n",
    "                                                                 medians,\n",
    "                                                                 data_no_quant,\n",
    "                                                                 num_neighbours = num_neighbours,\n",
    "                                                                 dist_metrics=dist_metrics)['10NN'])[0]\n",
    "\n",
    "                # computing intersection of the two neighbour sets\n",
    "                neighbours_intersect = set(neighbours_all_metrics).intersection(neighbours_metric_missing)    \n",
    "\n",
    "                \n",
    "                # computing proportion of neighbours shared between both sets\n",
    "                sim = len(neighbours_intersect)/num_neighbours\n",
    "\n",
    "                if not metric in prop_similarity:\n",
    "                    prop_similarity[metric] = [sim]\n",
    "                else:\n",
    "                    prop_similarity[metric].append(sim)\n",
    "                    \n",
    "# computing average proportion of similarity, per metric\n",
    "\n",
    "avg_prop_similarity = {}\n",
    "\n",
    "for metric in prop_similarity.keys():\n",
    "    avg_prop_similarity[metric] = mean(prop_similarity[metric])\n",
    "    \n",
    "sim_vals = list(avg_prop_similarity.values())\n",
    "\n",
    "# plotting similarity scores across interest and country combinations\n",
    "\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.set_title('Similarity Score')\n",
    "ax1.hist(sim_vals)\n",
    "ax1.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6716ca8e",
   "metadata": {},
   "source": [
    "### 8.2. Code used to generate Figure 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bf3429",
   "metadata": {},
   "source": [
    "```python\n",
    "scores = []\n",
    "\n",
    "# randomly choosing 100 interest combinations\n",
    "# each interest combination is essentially a list of 15 Booleans, 1 per interest\n",
    "# each list of Booleans is used to create an interested dict two code blocks down\n",
    "\n",
    "interest_combs = list(itertools.product([False, True], repeat=15))\n",
    "\n",
    "interest_combs_chosen = random.sample(interest_combs, 100)\n",
    "\n",
    "while [False] * 15 in interest_combs_chosen:\n",
    "    interest_combs_chosen = random.sample(interest_combs, 100)\n",
    "    \n",
    "# generating all region combinations\n",
    "\n",
    "region_combs = list(more_itertools.powerset(regions))\n",
    "\n",
    "interests = [\"covid\", \n",
    "             \"infrastructure_quality_and_availability\",\n",
    "             \"health_and_safety\",\n",
    "             \"cost\",\n",
    "             \"fun\",\n",
    "             \"nature\",\n",
    "             \"food\",\n",
    "             \"museums\",\n",
    "             \"showstheatresandmusic\",\n",
    "             \"wellness\",\n",
    "             \"wildlife\"]\n",
    "\n",
    "for interest_comb in interest_combs_chosen:  \n",
    "    for regions in region_combs:\n",
    "        # generating the interested dict corresponding to the given interest combination, interest_comb\n",
    "        interested = {}\n",
    "        for i, interest in enumerate(interests):\n",
    "            interested[interest] = interest_comb[i]\n",
    "            \n",
    "        # computing silhouette score, for given sets of interests and regions\n",
    "        if list(interested.values()).count(True) != 0 and len(regions) != 0:\n",
    "            weightings = convert_interests_to_col_weightings(interested)\n",
    "            medians_scaled_pca, medians_scaled = prepare_data_for_clustering(countries_data, regions, weightings)\n",
    "\n",
    "            scores.append(compute_silhouette_score(medians_scaled_pca))\n",
    "            \n",
    "# plotting silhouette scores across interest and region combinations\n",
    "\n",
    "fig2, ax2 = plt.subplots()\n",
    "ax2.set_title('Silhouette Score')\n",
    "ax2.boxplot(scores, vert=False)\n",
    "plt.show()      \n",
    "```"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
